{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"A very simple MNIST classifier.\n",
    "See extensive documentation at\n",
    "https://www.tensorflow.org/get_started/mnist/beginners\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "K.image_data_format() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.layers.core.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)                       \n",
    "units：大于0的整数，代表该层的输出维度。                                                                                                \n",
    "activation：激活函数，为预定义的激活函数名（参考激活函数），或逐元素（element-wise）的Theano函数。如果不指定该参数，将不会使用任何激活函数（即使用线性激活函数：a(x)=x）\n",
    "use_bias: 布尔值，是否使用偏置项\n",
    "kernel_initializer：权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考initializers                        \n",
    "bias_initializer：偏置向量初始化方法，为预定义初始化方法名的字符串，或用于初始化偏置向量的初始化器。参考initializers                  \n",
    "kernel_regularizer：施加在权重上的正则项，为Regularizer对象                                                                            \n",
    "bias_regularizer：施加在偏置向量上的正则项，为Regularizer对象                                                                          \n",
    "activity_regularizer：施加在输出上的正则项，为Regularizer对象                                                                          \n",
    "kernel_constraints：施加在权重上的约束项，为Constraints对象                                                                            \n",
    "bias_constraints：施加在偏置上的约束项，为Constraints对象                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /input_data\\train-images-idx3-ubyte.gz\n",
      "Extracting /input_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting /input_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /input_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#定义文件的位置\n",
    "data_dir = '/input_data'\n",
    "#读取并且下载\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=True)\n",
    "\n",
    "#定义一个占位符  代表x轴\n",
    "x = tf.placeholder(tf.float32, [None, 784])#784 是图片28*28的列\n",
    "#tf.float32  代表值的类型\n",
    "#[None, 784]  行数未确定, 这里有784列\n",
    "\n",
    "#定义一个占位符   代表预测的理想值\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])#10   是要出的理想结果\n",
    "#tf.float32 代表值的类型\n",
    "#[None, 10] 行数未确定,有10列   \n",
    "\n",
    "#定义一个占位符   代表学习率\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "#tf.float32  代表值的类型\n",
    "\n",
    "#定义操作的名字\n",
    "with tf.name_scope('reshape'):\n",
    "  #定义张量积\n",
    "  x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "  #-1  代表未知道输入的数目，这个可以变\n",
    "  #28  代表图片的长为28\n",
    "  #28  代表图片的宽为28\n",
    "  #1   输出一个核\n",
    "\n",
    "#定义卷积网络 \n",
    "net = Conv2D(32, kernel_size=[5,5], strides=[1,1],activation='relu',padding='same',input_shape=[28,28,1])(x_image)\n",
    "#32  指定的卷积核个数\n",
    "#kernel_size=[5,5]  卷积核的大小\n",
    "#strides=[1,1]  步长\n",
    "#activation='relu'  激活的函数\n",
    "#padding='same'  填充\n",
    "#input_shape=[28,28,1]  读入的张量积\n",
    "\n",
    "#定义最大池化\n",
    "net = MaxPooling2D(pool_size=[4,4])(net)\n",
    "#pool_size=[2,2]  定义池的大小\n",
    "\n",
    "#定义卷积网络\n",
    "net = Conv2D(64, kernel_size=[5,5], strides=[1,1],activation='relu',padding='same')(net)\n",
    "#64  指定的卷积核个数\n",
    "#kernel_size=[5,5]  卷积核的大小\n",
    "#strides=[1,1]  步长\n",
    "#activation='relu'  激活的函数\n",
    "#padding='same'  填充\n",
    "\n",
    "#定义最大池化\n",
    "net = MaxPooling2D(pool_size=[3,3])(net)\n",
    "#pool_size=[4,4]  定义池的大小\n",
    "\n",
    "#定义卷积网络\n",
    "net = Conv2D(128, kernel_size=[5,5], strides=[1,1],activation='relu',padding='same')(net)\n",
    "#128  指定的卷积核个数\n",
    "#kernel_size=[5,5]  卷积核的大小\n",
    "#strides=[1,1]  步长\n",
    "#activation='relu'  激活的函数\n",
    "#padding='same'  填充\n",
    "\n",
    "#定义最大池化\n",
    "net = MaxPooling2D(pool_size=[2,2])(net)\n",
    "#pool_size=[2,2]  定义池的大小\n",
    "\n",
    "\n",
    "\n",
    "#压平池化\n",
    "net = Flatten()(net)\n",
    "\n",
    "#定义全连接\n",
    "net = Dense(1000, activation='relu')(net)\n",
    "#1000  输出的维度\n",
    "#activation='relu'  激活函数\n",
    "\n",
    "#定义全连接\n",
    "net = Dense(10,activation='softmax')(net)\n",
    "#10  输出的维度\n",
    "#activation='softmax'  激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.objectives import categorical_crossentropy\n",
    "#定义出交叉熵的平均值\n",
    "cross_entropy = tf.reduce_mean(categorical_crossentropy(y_, net))\n",
    "#categorical_crossentropy(y_, net)  算出交叉熵\n",
    "#y_  理想的y值\n",
    "#net  预测的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义l2损失\n",
    "l2_loss = tf.add_n( [tf.nn.l2_loss(w) for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)] )\n",
    "#算出总的损失\n",
    "total_loss = cross_entropy + 7e-5*l2_loss\n",
    "#定义梯度下降的对象\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "#learning_rate  学习率\n",
    "#total_loss   总的损失\n",
    "\n",
    "#定义一个session对象\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把kears设置都session中\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, entropy loss: 2.300767, l2_loss: 1228.569946, total loss: 2.386767\n",
      "0.24\n",
      "step 200, entropy loss: 2.292411, l2_loss: 1228.422607, total loss: 2.378401\n",
      "0.26\n",
      "step 300, entropy loss: 2.289448, l2_loss: 1228.292236, total loss: 2.375429\n",
      "0.29\n",
      "step 400, entropy loss: 2.280839, l2_loss: 1228.184448, total loss: 2.366812\n",
      "0.31\n",
      "step 500, entropy loss: 2.272893, l2_loss: 1228.105835, total loss: 2.358860\n",
      "0.41\n",
      "step 600, entropy loss: 2.268124, l2_loss: 1228.073730, total loss: 2.354089\n",
      "0.42\n",
      "step 700, entropy loss: 2.245479, l2_loss: 1228.122314, total loss: 2.331447\n",
      "0.38\n",
      "step 800, entropy loss: 2.190274, l2_loss: 1228.311035, total loss: 2.276256\n",
      "0.5\n",
      "step 900, entropy loss: 2.093399, l2_loss: 1228.796875, total loss: 2.179415\n",
      "0.5\n",
      "step 1000, entropy loss: 1.785847, l2_loss: 1229.989990, total loss: 1.871947\n",
      "0.55\n",
      "0.5651\n",
      "step 1100, entropy loss: 1.404231, l2_loss: 1232.016235, total loss: 1.490472\n",
      "0.7\n",
      "step 1200, entropy loss: 1.074254, l2_loss: 1234.106201, total loss: 1.160641\n",
      "0.63\n",
      "step 1300, entropy loss: 0.801550, l2_loss: 1235.691406, total loss: 0.888048\n",
      "0.81\n",
      "step 1400, entropy loss: 0.653237, l2_loss: 1236.795654, total loss: 0.739813\n",
      "0.8\n",
      "step 1500, entropy loss: 0.407059, l2_loss: 1237.691650, total loss: 0.493697\n",
      "0.89\n",
      "step 1600, entropy loss: 0.437925, l2_loss: 1238.448730, total loss: 0.524616\n",
      "0.87\n",
      "step 1700, entropy loss: 0.375600, l2_loss: 1239.146240, total loss: 0.462340\n",
      "0.94\n",
      "step 1800, entropy loss: 0.284551, l2_loss: 1239.757446, total loss: 0.371334\n",
      "0.94\n",
      "step 1900, entropy loss: 0.303540, l2_loss: 1240.328125, total loss: 0.390363\n",
      "0.93\n",
      "step 2000, entropy loss: 0.208214, l2_loss: 1240.716064, total loss: 0.295064\n",
      "0.93\n",
      "0.9335\n",
      "step 2100, entropy loss: 0.186497, l2_loss: 1241.130981, total loss: 0.273377\n",
      "0.95\n",
      "step 2200, entropy loss: 0.149655, l2_loss: 1241.485229, total loss: 0.236559\n",
      "0.97\n",
      "step 2300, entropy loss: 0.381305, l2_loss: 1241.745117, total loss: 0.468227\n",
      "0.92\n",
      "step 2400, entropy loss: 0.182573, l2_loss: 1242.070801, total loss: 0.269518\n",
      "0.97\n",
      "step 2500, entropy loss: 0.216824, l2_loss: 1242.297119, total loss: 0.303785\n",
      "0.96\n",
      "step 2600, entropy loss: 0.169469, l2_loss: 1242.514526, total loss: 0.256445\n",
      "0.94\n",
      "step 2700, entropy loss: 0.114182, l2_loss: 1242.692749, total loss: 0.201171\n",
      "0.98\n",
      "step 2800, entropy loss: 0.132723, l2_loss: 1242.887329, total loss: 0.219725\n",
      "0.97\n",
      "step 2900, entropy loss: 0.141460, l2_loss: 1243.014404, total loss: 0.228471\n",
      "0.96\n",
      "step 3000, entropy loss: 0.095052, l2_loss: 1243.149902, total loss: 0.182073\n",
      "0.97\n",
      "0.9578\n",
      "step 3100, entropy loss: 0.091204, l2_loss: 1243.260986, total loss: 0.178233\n",
      "0.99\n",
      "step 3200, entropy loss: 0.132617, l2_loss: 1243.376099, total loss: 0.219653\n",
      "0.97\n",
      "step 3300, entropy loss: 0.098038, l2_loss: 1243.430664, total loss: 0.185078\n",
      "0.98\n",
      "step 3400, entropy loss: 0.185040, l2_loss: 1243.518188, total loss: 0.272087\n",
      "0.96\n",
      "step 3500, entropy loss: 0.063968, l2_loss: 1243.594727, total loss: 0.151019\n",
      "1.0\n",
      "step 3600, entropy loss: 0.111311, l2_loss: 1243.680298, total loss: 0.198369\n",
      "0.98\n",
      "step 3700, entropy loss: 0.102904, l2_loss: 1243.743530, total loss: 0.189966\n",
      "0.98\n",
      "step 3800, entropy loss: 0.105266, l2_loss: 1243.801514, total loss: 0.192332\n",
      "0.97\n",
      "step 3900, entropy loss: 0.138285, l2_loss: 1243.843628, total loss: 0.225354\n",
      "0.97\n",
      "step 4000, entropy loss: 0.118665, l2_loss: 1243.870361, total loss: 0.205736\n",
      "0.97\n",
      "0.9651\n",
      "step 4100, entropy loss: 0.052833, l2_loss: 1243.894531, total loss: 0.139906\n",
      "1.0\n",
      "step 4200, entropy loss: 0.100147, l2_loss: 1243.901855, total loss: 0.187220\n",
      "0.98\n",
      "step 4300, entropy loss: 0.089178, l2_loss: 1243.926270, total loss: 0.176253\n",
      "0.97\n",
      "step 4400, entropy loss: 0.056752, l2_loss: 1243.929321, total loss: 0.143827\n",
      "0.99\n",
      "step 4500, entropy loss: 0.029351, l2_loss: 1243.950562, total loss: 0.116428\n",
      "1.0\n",
      "step 4600, entropy loss: 0.048324, l2_loss: 1243.937500, total loss: 0.135400\n",
      "1.0\n",
      "step 4700, entropy loss: 0.094928, l2_loss: 1243.950562, total loss: 0.182005\n",
      "0.99\n",
      "step 4800, entropy loss: 0.070446, l2_loss: 1243.936279, total loss: 0.157522\n",
      "0.99\n",
      "step 4900, entropy loss: 0.167730, l2_loss: 1243.910645, total loss: 0.254803\n",
      "0.96\n",
      "step 5000, entropy loss: 0.029207, l2_loss: 1243.909546, total loss: 0.116281\n",
      "1.0\n",
      "0.9758\n"
     ]
    }
   ],
   "source": [
    "#初始化所有的值\n",
    "init_op = tf.global_variables_initializer()\n",
    "#运行\n",
    "sess.run(init_op)\n",
    "# 进行训练\n",
    "for step in range(5000):\n",
    "  #拿出训练集的x轴和y周\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  #100  拿100个节点\n",
    "  \n",
    "  #定义学习率\n",
    "  lr = 0.01\n",
    "  \n",
    "  #运行\n",
    "  _, loss, l2_loss_value, total_loss_value = sess.run([train_step, cross_entropy, l2_loss, total_loss],feed_dict={x: batch_xs, y_: batch_ys, learning_rate:lr})\n",
    "  #train_step  梯度下降的对象\n",
    "  #cross_entropy  交叉熵\n",
    "  #l2_loss  L2损失\n",
    "  #total_loss总的损失\n",
    "  #feed_dict={x: batch_xs, y_: batch_ys, learning_rate:lr}\n",
    "  #x: batch_xs  x轴的训练集\n",
    "  #y_: batch_ys y轴的结果集\n",
    "  #learning_rate:lr  学习率\n",
    "    \n",
    "  if (step+1) % 100 == 0:\n",
    "    print('step %d, entropy loss: %f, l2_loss: %f, total loss: %f' % (step+1, loss, l2_loss_value, total_loss_value))\n",
    "    # Test trained model\n",
    "    correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(y_, 1))\n",
    "    #tf.argmax 是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值\n",
    "    #tf.equal 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)\n",
    "    \n",
    "    #算出平均值\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    #tf.cast   [True, False, True, True] 会变成 [1,0,1,1] ，取平均值后得到 0.75\n",
    "    \n",
    "     #算出训练集得分\n",
    "    print(sess.run(accuracy, feed_dict={x: batch_xs, y_: batch_ys}))\n",
    "  if (step+1) % 1000 == 0:\n",
    "    #算出测试集的得分\n",
    "    print(sess.run(accuracy, feed_dict={x: mnist.test.images,y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
